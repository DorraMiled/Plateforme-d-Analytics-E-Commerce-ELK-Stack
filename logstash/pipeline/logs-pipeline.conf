input {
  file {
    path => "/usr/share/logstash/data/*.csv"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/.sincedb_csv"
    type => "logs_csv"
    mode => "tail"
    file_completed_action => "log"
    file_completed_log_path => "/usr/share/logstash/data/completed.log"
  }
  
  file {
    path => "/usr/share/logstash/data/*.json"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/.sincedb_json"
    type => "logs_json"
    mode => "tail"
    codec => "json_lines"
    file_completed_action => "log"
    file_completed_log_path => "/usr/share/logstash/data/completed.log"
  }
}

filter {
  # Parse CSV logs
  if [type] == "logs_csv" {
    csv {
      separator => ","
      columns => ["Timestamp", "Level", "Service", "Message", "User"]
      skip_header => true
    }
    
    # Parse timestamp
    date {
      match => ["Timestamp", "ISO8601"]
      target => "@timestamp"
    }
    
    mutate {
      add_tag => ["csv", "logs"]
      remove_field => ["host", "path", "message"]
    }
  }
  
  # Parse JSON logs
  if [type] == "logs_json" {
    # JSON is already parsed by codec
    date {
      match => ["Timestamp", "ISO8601"]
      target => "@timestamp"
    }
    
    mutate {
      add_tag => ["json", "logs"]
      remove_field => ["host", "path"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "ecommerce-logs-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }
  
  stdout {
    codec => rubydebug {
      metadata => false
    }
  }
}
