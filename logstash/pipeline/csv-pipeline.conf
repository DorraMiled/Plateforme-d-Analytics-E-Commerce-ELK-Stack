# CSV Pipeline - Parse e-commerce CSV orders
input {
  file {
    path => "/data/ecommerce-orders.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    type => "csv_orders"
  }
}

filter {
  # Parse CSV
  if [type] == "csv_orders" {
    csv {
      separator => ","
      columns => ["timestamp", "order_id", "customer_id", "customer_name", "customer_email", "customer_ip", "customer_country", "customer_city", "product_id", "product_name", "product_category", "quantity", "unit_price", "total_amount", "payment_method", "order_status", "shipping_method"]
      skip_header => true
    }
    
    # Parse timestamp
    date {
      match => ["timestamp", "yyyy-MM-dd HH:mm:ss"]
      target => "@timestamp"
    }
    
    # Convert numeric fields
    mutate {
      convert => {
        "quantity" => "integer"
        "unit_price" => "float"
        "total_amount" => "float"
      }
      # Add tags
      add_tag => ["csv", "orders", "ecommerce"]
      # Add event type
      add_field => {
        "event_type" => "order_placed"
      }
      # Remove unwanted fields
      remove_field => ["host", "path", "message"]
    }
    
    # Parse IP address for geolocation
    if [customer_ip] {
      geoip {
        source => "customer_ip"
        target => "geoip"
      }
    }
  }
}

output {
  if [type] == "csv_orders" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ecommerce-logs-%{+YYYY.MM.dd}"
      document_type => "_doc"
    }
    
    stdout {
      codec => rubydebug {
        metadata => false
      }
    }
  }
}
